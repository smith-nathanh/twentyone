# source: https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html

import random
from collections import namedtuple, deque
import Project2_env as environment
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F


class ReplayMemory(object):

    def __init__(self, capacity):
        self.memory = deque([], maxlen=capacity)

    def push(self, Transition, *args):
        """Save a transition"""
        self.memory.append(Transition(*args))

    def sample(self, batch_size):
        return random.sample(self.memory, batch_size)

    def __len__(self):
        return len(self.memory)


class DQN(nn.Module):
    """
    the DQN class, with two hidden layers each of 128 nodes.  All weights and biases
    are initialized at random using nn.Linear.
    """

    def __init__(self, n_observations, n_actions):
        super(DQN, self).__init__()
        self.layer1 = nn.Linear(n_observations, 128)
        self.layer2 = nn.Linear(128, 128)
        self.layer3 = nn.Linear(128, n_actions)

    # Called with either one element to determine next action, or a batch
    # during optimization. Returns tensor([[left0exp,right0exp]...]).
    def forward(self, x):
        x = F.relu(self.layer1(x))
        x = F.relu(self.layer2(x))
        return self.layer3(x)


def get_agent_hand(state):
    """
    Convert state to player hand.
    :param state: the game state
    :return: the player hand (as a number 1 through 10 representing 12 through 21, since the agent auto-hits below 12),
    and 1 if there is a usable ace, 0 if not
    """
    return state % 10 + 1, int(state - 100 >= 0)


def get_dealer_hand(state):
    """
    Convert state to dealer face-up card.
    :param state: the game state
    :return: the dealer face-up card
    """
    return ((state - state % 10) / 10) % 10 + 1


def select_action(state, eps_threshold, policy_model, device):
    """
    returns an action selected with epsilon greedy
    :param state: a tensor of the current state
    :param eps_threshold: epsilon
    :param policy_model: the policy network
    :param device: cpu ot gpu
    :return: the chosen action
    """
    sample = random.random()
    if sample > eps_threshold:
        with torch.no_grad(): # temporarily disable gradient calculation for faster computation
            # t.max(1) will return the largest column value of each row.
            # second column on max result is index of where max element was
            # found, so we pick action with the larger expected reward.
            # note that in PyTorch, you don't need to explicitly call model.forward, which is why it is omitted here.
            return policy_model(state).max(1).indices.view(1, 1)
    else:
        return torch.tensor([[random.randint(0,1)]], device=device, dtype=torch.long)


def optimize_model(memory, BATCH_SIZE, Transition, device, policy_net, target_net, GAMMA, optimizer):
    if len(memory) < BATCH_SIZE:
        return
    transitions = memory.sample(BATCH_SIZE)
    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for
    # detailed explanation). This converts batch-array of Transitions
    # to Transition of batch-arrays.
    batch = Transition(*zip(*transitions))

    # Compute a mask of non-final states and concatenate the batch elements
    # (a final state is any that ends a hand)
    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool)
    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])
    state_batch = torch.cat(batch.state)
    action_batch = torch.cat(batch.action)
    reward_batch = torch.cat(batch.reward)

    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the
    # columns of actions taken. These are the actions which would've been taken
    # for each batch state according to policy_net
    state_action_values = policy_net(state_batch).gather(1, action_batch)

    # Compute V(s_{t+1}) for all next states.
    # Expected values of actions for non_final_next_states are computed based
    # on the "older" target_net; selecting their best reward with max(1).values
    # This is merged based on the mask, such that we'll have either the expected
    # state value or 0 in case the state was final.
    next_state_values = torch.zeros(BATCH_SIZE, device=device)
    with torch.no_grad():
        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1).values
    # Compute the expected Q values
    expected_state_action_values = (next_state_values * GAMMA) + reward_batch

    # Compute Huber loss
    criterion = nn.SmoothL1Loss()
    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))

    # Optimize the model
    optimizer.zero_grad()
    loss.backward()

    # In-place gradient clipping
    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)
    optimizer.step()


def adjust_count(cards):
    """
    calculate the adjustment to the hi_lo count from the latest cards
    :param cards: the latest open cards
    :return: the adjustment
    """
    count = 0
    for card in cards:
        if card == 1 or card == 10:
            count -= 1
        elif 2 <= card <= 6:
            count += 1
    return count


def get_true_count(hi_lo, decks_remaining):
    true_count = round(hi_lo/decks_remaining) + 15
    true_count = max(true_count, 0)
    true_count = min(true_count, 29)

    return true_count


def create_tensor_state(deck_state, agent_hand, agent_ace, dealer_hand, betting_phase, device):
    starting_state = np.concatenate((deck_state, [agent_hand], [agent_ace], [dealer_hand], [betting_phase]))
    return torch.tensor(starting_state, dtype=torch.float32, device=device).unsqueeze(0)


def process_transition(reward, starting_state, action, next_state, device, policy_net, target_net, TAU, memory, BATCH_SIZE, Transition, GAMMA, optimizer):
    """
    push the latest transition to the memory bank, optimize the policy weights, then update the target weights to get slightly closer to the policy weights
    :param reward:
    :param starting_state:
    :param action:
    :param next_state:
    :param device:
    :param policy_net:
    :param target_net:
    :param TAU:
    :param memory:
    :param BATCH_SIZE:
    :param Transition:
    :param GAMMA:
    :param optimizer:
    :return:
    """
    reward = torch.tensor([reward], device=device)
    memory.push(Transition, starting_state, action, next_state, reward)
    optimize_model(memory, BATCH_SIZE, Transition, device, policy_net, target_net, GAMMA, optimizer)
    target_net_state_dict = target_net.state_dict()
    policy_net_state_dict = policy_net.state_dict()
    for key in policy_net_state_dict:
        target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)
    target_net.load_state_dict(target_net_state_dict)


def play_blackjack(env, model, target_net, device):

    Transition = namedtuple('Transition',('state', 'action', 'next_state', 'reward'))

    # BATCH_SIZE is the number of transitions sampled from the replay buffer
    # GAMMA is the discount factor as mentioned in the previous section
    # TAU is the update rate of the target network
    # LR is the learning rate of the ``AdamW`` optimizer
    BATCH_SIZE = 128
    GAMMA = 0.99
    TAU = 0.005
    LR = 1e-4
    eps_hit_stick = 0.2
    eps_bet = 0.5
    num_episodes = 2000

    policy_net = model.to(device)
    target_net.load_state_dict(policy_net.state_dict())

    optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)
    memory = ReplayMemory(10000)

    total_return = 0
    hand_count = 0
    cur_episode = 0

    while True:
        hand_count += 1
        deck_state = np.array(env.get_card_state(), dtype=float)/96
        starting_state = create_tensor_state(deck_state, 0, 0, 0, 1, device)
        action = select_action(starting_state, eps_bet, policy_net, device)

        bet_size = 1
        if action.item() == 1:
            bet_size = 10

        hand_state, open_cards = env.reset()

        if hand_state == 203:
            reward = 1.5 * bet_size
            total_return += reward
            process_transition(reward, starting_state, action, None, device, policy_net, target_net, TAU, memory, BATCH_SIZE, Transition, GAMMA, optimizer)

        opening_action = True
        agent_hand, agent_ace = get_agent_hand(hand_state)
        dealer_hand = get_dealer_hand(hand_state)
        deck_state = np.array(env.get_card_state(), dtype=float)/96
        state = create_tensor_state(deck_state, agent_hand/10, agent_ace, dealer_hand/10, 0, device)

        while hand_state < 200:
            if opening_action:
                opening_action = False
                reward = torch.tensor([0], device=device)
                process_transition(reward, starting_state, action, state, device, policy_net, target_net, TAU, memory, BATCH_SIZE, Transition, GAMMA, optimizer)

            action = select_action(state, eps_hit_stick, policy_net, device)

            hand_state, reward, open_cards = env.execute_action(action)

            reward *= bet_size
            total_return += reward
            reward = torch.tensor([reward], device=device)
            agent_hand, agent_ace_new = get_agent_hand(hand_state)
            deck_state = np.array(env.get_card_state(), dtype=float)
            next_state = create_tensor_state(deck_state/96, agent_hand/10, agent_ace, dealer_hand/10, 0, device)
            process_transition(reward, state, action, next_state, device, policy_net, target_net, TAU, memory, BATCH_SIZE, Transition, GAMMA, optimizer)

            # Move to the next state
            state = next_state

        if len(env.deck.cards) < 125:
            cur_episode += 1
            if cur_episode > num_episodes:
                break
            total_return = 0
            hand_count = 0
            env = environment.Blackjack()

    torch.save(policy_net.state_dict(), 'dqn_model_bet150_policy.pth')  # Saves the PyTorch model weights
    torch.save(policy_net.state_dict(), 'dqn_model_bet150_target.pth')  # Saves the PyTorch model weights

    print('Complete')


def reload_model(env, model, device, print_hi_lo):
    model.load_state_dict(torch.load('dqn_model_bet150_policy.pth'))
    eps_threshold = 0
    num_episodes = 1000
    total_return = 0
    hand_count = 0
    cur_episode = 0
    hi_lo = 0
    arr_return = []

    with open('output_9.txt', 'w') as file:
        if print_hi_lo:
            print('ep', 'hi_lo', 'bet', sep=',', file=file)
        else:
            print('ep', 'agent', 'ace', 'dealer', 'bet', sep=',', file=file)

        while True:
            true_count = get_true_count(hi_lo, (len(env.deck.cards)/52))
            hand_count += 1
            deck_state = np.array(env.get_card_state(), dtype=float)/96
            starting_state = create_tensor_state(deck_state, 0, 0, 0, 1, device)
            action = select_action(starting_state, eps_threshold, model, device)

            bet_size = 1
            if action.item() == 1:
                bet_size = 10

            hand_state, open_cards = env.reset()
            hi_lo += adjust_count(open_cards)

            if print_hi_lo:
                print(cur_episode, true_count, bet_size, sep=',', file=file)

            if hand_state == 203:
                reward = 1.5 * bet_size
                total_return += reward

            opening_action = True
            agent_hand, agent_ace = get_agent_hand(hand_state)
            dealer_hand = get_dealer_hand(hand_state)
            deck_state = np.array(env.get_card_state(), dtype=float)/96
            state = create_tensor_state(deck_state, agent_hand/10, agent_ace, dealer_hand/10, 0, device)

            while hand_state < 200:
                if opening_action:
                    opening_action = False

                action = select_action(state, eps_threshold, model, device)

                if not print_hi_lo:
                    print(cur_episode, agent_hand+11, agent_ace, dealer_hand, action.item(), sep=',', file=file)

                hand_state, reward, open_cards = env.execute_action(action)
                hi_lo += adjust_count(open_cards)

                reward *= bet_size
                total_return += reward
                agent_hand, agent_ace_new = get_agent_hand(hand_state)
                deck_state = np.array(env.get_card_state(), dtype=float)
                next_state = create_tensor_state(deck_state/96, agent_hand/10, agent_ace, dealer_hand/10, 0, device)

                # Move to the next state
                state = next_state

            if len(env.deck.cards) < 125:
                arr_return.append(total_return/hand_count)
                print(cur_episode, total_return/hand_count, sep=',')
                cur_episode += 1
                if cur_episode >= num_episodes:
                    break
                total_return = 0
                hand_count = 0
                env = environment.Blackjack()
                hi_lo = 0


def play_game(is_reload_model):
    env = environment.Blackjack()
    n_actions = 2
    n_observations = 14
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    policy_net = DQN(n_observations, n_actions)
    target_net = DQN(n_observations, n_actions)

    if is_reload_model:
        reload_model(env, policy_net, device, True)

    else:
        #model.load_state_dict(torch.load('dqn_model_bet150.pth'))
        #model_target.load_state_dict(torch.load('dqn_model_bet150.pth'))
        play_blackjack(env, policy_net, target_net, device)


play_game(True)

